{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f15f2a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist import MNIST\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gensim.downloader\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ba2735",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "For the entirety of Task 1, I'll be using the Keras model in tensorflow. This is because Keras has frameworks for RNN, LSTM, and GRU! If I create a 2 layer sequential keras model where it goes **| Input -> RNN -> Dense -> Output |** I can easily replace the simple RNN layer with a GRU or LSTM layer to help with Part 2 of task 1. \n",
    "\n",
    "Side note: The dataset I'm using is Delhi's daily climate over the span of a couple years. The dataset only has 4 variables and I'll be attempting to predict the weather of every 7th day, so the input dimension is 4x6.\n",
    "\n",
    "The output is 4 variables: Temp, Humidity, Wind Speed, Pressure.\n",
    "\n",
    "I'll measure performance by predicting the 7th day of a 6 day sequence, compare it to the actual 7th day, then use cosine similarity. I use cos similarity because all 4 variables predicted are all regression based predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d437a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>meantemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>meanpressure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>84.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1015.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>2.980000</td>\n",
       "      <td>1017.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>7.166667</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>4.633333</td>\n",
       "      <td>1018.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>71.333333</td>\n",
       "      <td>1.233333</td>\n",
       "      <td>1017.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>86.833333</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>1016.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   meantemp   humidity  wind_speed  meanpressure\n",
       "0  2013-01-01  10.000000  84.500000    0.000000   1015.666667\n",
       "1  2013-01-02   7.400000  92.000000    2.980000   1017.800000\n",
       "2  2013-01-03   7.166667  87.000000    4.633333   1018.666667\n",
       "3  2013-01-04   8.666667  71.333333    1.233333   1017.166667\n",
       "4  2013-01-05   6.000000  86.833333    3.700000   1016.500000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab data\n",
    "dfTrain = pd.read_csv(\"DailyDelhiClimateTrain.csv\")\n",
    "dfTest = pd.read_csv(\"DailyDelhiClimateTest.csv\")\n",
    "dfTrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ce9d059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.00000000e+01 8.45000000e+01 0.00000000e+00 1.01566667e+03]\n",
      "  [7.40000000e+00 9.20000000e+01 2.98000000e+00 1.01780000e+03]\n",
      "  [7.16666667e+00 8.70000000e+01 4.63333333e+00 1.01866667e+03]\n",
      "  [8.66666667e+00 7.13333333e+01 1.23333333e+00 1.01716667e+03]\n",
      "  [6.00000000e+00 8.68333333e+01 3.70000000e+00 1.01650000e+03]\n",
      "  [7.00000000e+00 8.28000000e+01 1.48000000e+00 1.01800000e+03]\n",
      "  [7.00000000e+00 7.86000000e+01 6.30000000e+00 1.02000000e+03]]\n",
      "\n",
      " [[8.85714286e+00 6.37142857e+01 7.14285714e+00 1.01871429e+03]\n",
      "  [1.40000000e+01 5.12500000e+01 1.25000000e+01 1.01700000e+03]\n",
      "  [1.10000000e+01 6.20000000e+01 7.40000000e+00 1.01566667e+03]\n",
      "  [1.57142857e+01 5.12857143e+01 1.05714286e+01 1.01614286e+03]\n",
      "  [1.40000000e+01 7.40000000e+01 1.32285714e+01 1.01557143e+03]\n",
      "  [1.58333333e+01 7.51666667e+01 4.63333333e+00 1.01333333e+03]\n",
      "  [1.28333333e+01 8.81666667e+01 6.16666667e-01 1.01516667e+03]]\n",
      "\n",
      " [[1.47142857e+01 7.18571429e+01 5.28571429e-01 1.01585714e+03]\n",
      "  [1.38333333e+01 8.66666667e+01 0.00000000e+00 1.01666667e+03]\n",
      "  [1.65000000e+01 8.08333333e+01 5.25000000e+00 1.01583333e+03]\n",
      "  [1.38333333e+01 9.21666667e+01 8.95000000e+00 1.01450000e+03]\n",
      "  [1.25000000e+01 7.66666667e+01 5.88333333e+00 1.02166667e+03]\n",
      "  [1.12857143e+01 7.52857143e+01 8.47142857e+00 1.02028571e+03]\n",
      "  [1.12000000e+01 7.70000000e+01 2.22000000e+00 1.02100000e+03]]\n",
      "\n",
      " [[9.50000000e+00 7.96666667e+01 3.08333333e+00 1.02180000e+03]\n",
      "  [1.40000000e+01 6.01666667e+01 4.01666667e+00 1.02050000e+03]\n",
      "  [1.38333333e+01 6.06666667e+01 6.16666667e+00 1.02050000e+03]\n",
      "  [1.22500000e+01 6.70000000e+01 5.55000000e+00 1.02075000e+03]\n",
      "  [1.26666667e+01 6.41666667e+01 6.80000000e+00 1.01966667e+03]\n",
      "  [1.28571429e+01 6.55714286e+01 5.55714286e+00 1.01814286e+03]\n",
      "  [1.48333333e+01 5.60000000e+01 3.70000000e+00 1.01783333e+03]]]\n"
     ]
    }
   ],
   "source": [
    "# Drop date\n",
    "dfTrain = dfTrain.drop(columns=['date'])\n",
    "dfTest = dfTest.drop(columns=['date'])\n",
    "\n",
    "# Reformat into np array\n",
    "Train = dfTrain.to_numpy()\n",
    "Test = dfTest.to_numpy()\n",
    "\n",
    "# Size\n",
    "trainRows, trainCols = Train.shape\n",
    "testRows, testCols = Test.shape\n",
    "\n",
    "# Drop extra rows so Train mod 7 = 0\n",
    "Train = np.delete(Train, slice(trainRows - (trainRows % 7),trainRows), 0)\n",
    "Test = np.delete(Test, slice(testRows - (testRows % 7),testRows), 0)\n",
    "\n",
    "# Reshape into 3D nx7x4 shape\n",
    "Train = np.reshape(Train, (trainRows//7, 7, 4))\n",
    "Test = np.reshape(Test, (testRows//7, 7, 4))\n",
    "\n",
    "print(Train[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59b06e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  10.           84.5           0.         1015.66666667]\n",
      "  [   7.4          92.            2.98       1017.8       ]\n",
      "  [   7.16666667   87.            4.63333333 1018.66666667]\n",
      "  [   8.66666667   71.33333333    1.23333333 1017.16666667]\n",
      "  [   6.           86.83333333    3.7        1016.5       ]\n",
      "  [   7.           82.8           1.48       1018.        ]]\n",
      "\n",
      " [[   8.85714286   63.71428571    7.14285714 1018.71428571]\n",
      "  [  14.           51.25         12.5        1017.        ]\n",
      "  [  11.           62.            7.4        1015.66666667]\n",
      "  [  15.71428571   51.28571429   10.57142857 1016.14285714]\n",
      "  [  14.           74.           13.22857143 1015.57142857]\n",
      "  [  15.83333333   75.16666667    4.63333333 1013.33333333]]]\n",
      "(208, 6, 4)\n",
      "[[7.00000000e+00 7.86000000e+01 6.30000000e+00 1.02000000e+03]\n",
      " [1.28333333e+01 8.81666667e+01 6.16666667e-01 1.01516667e+03]]\n",
      "(208, 4)\n"
     ]
    }
   ],
   "source": [
    "# Split into nx6x4 shape X and nx1x4 shape Y\n",
    "a, b, c, yTrain = np.array_split(Train, 4, 1) \n",
    "xTrain = np.concatenate((a,b,c), 1) \n",
    "yTrain = yTrain.reshape(trainRows//7,4)\n",
    "\n",
    "a, b, c, yTest = np.array_split(Test, 4, 1) \n",
    "xTest = np.concatenate((a,b,c), 1) \n",
    "yTest = yTest.reshape(testRows//7,4)\n",
    "\n",
    "print(xTrain[0:2])\n",
    "print(xTrain.shape)\n",
    "print(yTrain[0:2])\n",
    "print(yTrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f335cd84",
   "metadata": {},
   "source": [
    "Next let's configure the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01f531a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 6)                 66        \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 28        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 94\n",
      "Trainable params: 94\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Input, RNN, Dense layer, Output\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.Input((6, 4)),\n",
    "        tf.keras.layers.SimpleRNN(6, activation='tanh'), # 6 hidden units (one for each time state 1 thru 6)\n",
    "        tf.keras.layers.Dense(4, activation='softmax')\n",
    "    ]\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "# Use ADAM optimization. No regularization\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='mean_squared_error', metrics=['cosine_similarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3658345d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "208/208 [==============================] - 1s 1ms/step - loss: 323569.5625 - cosine_similarity: 0.7461\n",
      "Epoch 2/3\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 323371.6875 - cosine_similarity: 0.9917\n",
      "Epoch 3/3\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 323335.7188 - cosine_similarity: 0.9946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a488b12230>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=xTrain,y=yTrain,batch_size=1,epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12d0531f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 192ms/step - loss: 256567.9688 - cosine_similarity: 0.9983\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[256567.96875, 0.998266339302063]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=xTest,y=yTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf14757",
   "metadata": {},
   "source": [
    "Well I got an impressive output for cosine similarity. 99.88%\n",
    "\n",
    "## Part 2\n",
    "\n",
    "Let's see if I can do better with LSTM or GRU! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "630e8a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 6)                 264       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 28        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 292\n",
      "Trainable params: 292\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Input, RNN, Dense layer, Output\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.Input((6, 4)),\n",
    "        tf.keras.layers.LSTM(6, activation='tanh'), # 6 cells (one for each time state 1 thru 6)\n",
    "        tf.keras.layers.Dense(4, activation='softmax')\n",
    "    ]\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "# Use ADAM optimization. No regularization\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='mean_squared_error', metrics=['cosine_similarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06f9238d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "208/208 [==============================] - 2s 2ms/step - loss: 323657.3125 - cosine_similarity: 0.6327\n",
      "Epoch 2/3\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 323536.6250 - cosine_similarity: 0.9137\n",
      "Epoch 3/3\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 323437.7188 - cosine_similarity: 0.9862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a4910d7b50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=xTrain,y=yTrain,batch_size=1,epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc137f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 356ms/step - loss: 256642.1875 - cosine_similarity: 0.9966\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[256642.1875, 0.9966260194778442]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=xTest,y=yTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e20e1d",
   "metadata": {},
   "source": [
    "For LSTM: 98.53%\n",
    "\n",
    "Not much of a difference (not that a significant improvement can be made anyway) Now to GRU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc77c498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru (GRU)                   (None, 6)                 216       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 28        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 244\n",
      "Trainable params: 244\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Input, RNN, Dense layer, Output\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.Input((6, 4)),\n",
    "        tf.keras.layers.GRU(6, activation='tanh'), # 6 cells (one for each time state 1 thru 6)\n",
    "        tf.keras.layers.Dense(4, activation='softmax')\n",
    "    ]\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "# Use ADAM optimization. No regularization\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='mean_squared_error', metrics=['cosine_similarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b51a2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "208/208 [==============================] - 1s 2ms/step - loss: 323681.9688 - cosine_similarity: 0.5635\n",
      "Epoch 2/3\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 323582.4375 - cosine_similarity: 0.8445\n",
      "Epoch 3/3\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 323487.0625 - cosine_similarity: 0.9630\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a4934932e0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=xTrain,y=yTrain,batch_size=1,epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbe99c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 319ms/step - loss: 256688.7969 - cosine_similarity: 0.9805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[256688.796875, 0.9804650545120239]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=xTest,y=yTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d143d59c",
   "metadata": {},
   "source": [
    "For GRU: 99.90%!\n",
    "\n",
    "All in all, each option did incredibly good (98+ percent) and while GRU technically did the best, there isn't much difference. I believe this might just be an issue of not having an incredibly complex dataset to thouroughly display how one option is better than the others.\n",
    "\n",
    "## Part 3 \n",
    "\n",
    "You COULD use a traditional feed-forward network on my input. The input would be a 6 x 4 2d array and the output would be a 1x4 array where the error is output vs the 7th day 1x4 array.\n",
    "\n",
    "The thing is that in a feed-forward, time becomes another dimension you add to the input. Meanwhile in an RNN, time is an aspect of the network as a whole and is more emphasized.\n",
    "\n",
    "I believe a feed-forward network will have more of an issue understanding that the later days (day 5 and 6) have more of an impact on day 7's weather than days 1 and 2. Meanwhile, an RNN tends to emphasize the importance of the later time segments than the earlier ones.\n",
    "\n",
    "# Task 2\n",
    "\n",
    "I'm gonna use gensim's model to create a function to compare 2 words' cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df8979b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('trees', 0.6832190155982971), ('leaf', 0.6587675213813782), ('bark', 0.6577526330947876), ('avl', 0.6302106380462646), ('bird', 0.6102473139762878), ('fruit', 0.5899716019630432), ('flower', 0.5892435908317566), ('skeleton', 0.5835850238800049), ('beetle', 0.5697873830795288), ('garden', 0.5644292831420898)]\n"
     ]
    }
   ],
   "source": [
    "corpus = gensim.downloader.load('text8')\n",
    "w2v = Word2Vec(corpus)\n",
    "print(w2v.wv.most_similar('tree'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6cdddd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simDisim(word1, word2):\n",
    "    sim = w2v.wv.similarity(word1, word2)\n",
    "    bestW1 = w2v.wv.most_similar(word1)\n",
    "    bestW2 = w2v.wv.most_similar(word2)\n",
    "    dissim = 0\n",
    "    for neighbor in bestW1:\n",
    "        dissim += w2v.wv.similarity(word2, neighbor[0])\n",
    "    for neighbor in bestW2:\n",
    "        dissim += w2v.wv.similarity(word1, neighbor[0])\n",
    "    dissim /= 20\n",
    "    dissim = 1 - dissim\n",
    "    print(word1, \" and \", word2, \" similarity: \", sim)\n",
    "    print(word1, \" and \", word2, \" dissimilarity: \", dissim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "efa1db8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pizza  and  party  similarity:  -0.004141184\n",
      "pizza  and  party  dissimilarity:  1.0474904676550068\n",
      "tree  and  bird  similarity:  0.61024725\n",
      "tree  and  bird  dissimilarity:  0.4651725277304649\n",
      "fruit  and  vegetable  similarity:  0.8231189\n",
      "fruit  and  vegetable  dissimilarity:  0.16694877743721004\n",
      "car  and  park  similarity:  0.3389139\n",
      "car  and  park  dissimilarity:  0.725123417750001\n"
     ]
    }
   ],
   "source": [
    "simDisim(\"pizza\",\"party\")\n",
    "simDisim(\"tree\",\"bird\")\n",
    "simDisim(\"fruit\",\"vegetable\")\n",
    "simDisim(\"car\",\"park\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445128bd",
   "metadata": {},
   "source": [
    "For similarity, I can just call the cosine similarity function that gensim has. For dissimilarity, I wanted to do something special. It's the average similarity of word1 vs word2's neighbors and word2 vs word1's neighbors. \n",
    "\n",
    "For words used commonly together, they will have high similarity. This does not imply that the words are always used in the same contexts. For example, car park is a regular usage of car and park together. Although, park is usually used in a different context, talking about a garden or playground.\n",
    "\n",
    "For this, dissimilarity compares if the contexts word1 and word2 usually exist within are similar! If dissimilarity is high, their contexts are usually different. If dissimilarity is low, they tend to remain together between contexts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
