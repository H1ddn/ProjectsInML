{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f15f2a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist import MNIST\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ba2735",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "To pick a good framework, I need to get a good dataset. Luckily, I've already got MNIST's Handwritten Digits dataset downloaded and since we've been talking about image recognition in class, I think this would be a great dataset.\n",
    "\n",
    "First thing I had to figure out what in the world a Tensor is. From my understanding, It's an advanced multidimensional array, like an array of matricies or something. Each layer in the NN is a tensor of weights. The input is a tensor as well! The output for me will be the classification. The data structure in tensorflow is called a '[Variable](https://www.tensorflow.org/api_docs/python/tf/Variable)'.\n",
    "\n",
    "The next thing I had to figure out was [Activation Functions](https://himanshuxd.medium.com/activation-functions-sigmoid-relu-leaky-relu-and-softmax-basics-for-neural-networks-and-deep-8d9c70eed91e). After looking through multple sources and guides on neural networks the question became less of What Activation Functions, but why use ReLU & softmax (as those were the most popular). The biggest explanation is that the other functions (tanh, sigmoid, etc) suffer from 'vanishing gradient problem' and 'exploding gradients' both during backpropogation. ReLU is routinely offered as a solution. Softmax is offered to solve vanishing gradient as well. Since it's only two layers such a problem will probably not happen, but once I increase the number of layers this will be important.\n",
    "\n",
    "For the framework then, we need something for simple image recognition. I wanted to work with Tensorflow since i've had some experience with it before. I decided to use the Keras API within TF since Keras is really user friendly and helps simplify the layering of the neural network.\n",
    "\n",
    "[The Keras sequential model](https://www.tensorflow.org/guide/keras/sequential_model) is perfect for the dataset since it works layer-by-layer. My model will be a two-layer NN and I don't intend for layers output anywhere but the next layer.\n",
    "\n",
    "For the first layer, I decided to spice things up and add a [2D Convolution](https://www.geeksforgeeks.org/keras-conv2d-class/) layer. We discussed how the filtering with kernels work to add context to the image in class. I thought it might help to add a colvolution layer, but since we only get two layers to work with, this might actually hurt. The second layer will be a [Dense Layer](https://keras.io/api/layers/core_layers/dense/).\n",
    "\n",
    "After creating the keras model, you need to [compile](https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile) to configure the settings for backprop on the NN. By default, the model's optimizer is RMSprop, but I am going to set it to [ADAM](https://keras.io/api/optimizers/adam/) since that's what I implemented in HW1 and it went pretty well.\n",
    "\n",
    "## Task 2\n",
    "\n",
    "For the MNIST data, it's already very well organized so little is needed to clean the data. Although, I do want to mention I will be using the mnist data parser. I will however need to split Train and Dev."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d437a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab data\n",
    "data = MNIST('MNIST/raw')\n",
    "xDat, yDat = data.load_training()\n",
    "xTest, yTest = data.load_testing()\n",
    "\n",
    "# normalize & send to np array\n",
    "xDat = np.divide(np.array(xDat), 255.0)\n",
    "yDat = tf.keras.utils.to_categorical(np.array(yDat), 10)\n",
    "xTest = np.divide(np.array(xTest), 255.0)\n",
    "yTest = tf.keras.utils.to_categorical(np.array(yTest), 10)\n",
    "\n",
    "# Make xDev & yDev be the last 10% of data points in Dat\n",
    "size = yDat.size//100\n",
    "xTrain, xDev = xDat[size:,:], xDat[:size,:]\n",
    "yTrain, yDev = yDat[size:,:], yDat[:size,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f335cd84",
   "metadata": {},
   "source": [
    "Next let's configure the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01f531a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 64)        640       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 24, 24, 32)        18464     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 22, 22, 8)         2312      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3872)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               495744    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 518,450\n",
      "Trainable params: 518,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# reshape from 784 to 28x28\n",
    "xTrain = np.reshape(xTrain,(-1, 28, 28, 1))\n",
    "xDev = np.reshape(xDev,(-1, 28, 28, 1))\n",
    "xTest = np.reshape(xTest,(-1, 28, 28, 1))\n",
    "\n",
    "# Original 2 layer Model:\n",
    "#model = tf.keras.Sequential(\n",
    "#    [\n",
    "#        tf.keras.layers.Dense(128, activation='relu'),\n",
    "#        tf.keras.layers.Dense(10, activation='softmax')\n",
    "#    ]\n",
    "#)\n",
    "\n",
    "# 3 layers, a 2d Convolution layer, flatten, then a dense matrix\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.Input((28, 28, 1)),\n",
    "        tf.keras.layers.Conv2D(64, (3,3), activation='relu'), # default stride is 1,1\n",
    "        tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "        tf.keras.layers.Conv2D(8, (3,3), activation='relu'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ]\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "# Use ADAM optimization. No regularization\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47aec67",
   "metadata": {},
   "source": [
    "When it comes to forward prop and backprop we're lucky, Keras does it automatically! Similar to scikit, there is a fit function which acts as a train. Keras will automagically calculate loss using mean squared error and backprop to shift the weights. \n",
    "\n",
    "On fit, I will select batches as 32 and epochs at 2. Since the train set is 50k samples, this means there is ~1500 batches total. About 3000 32 size backprops will occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3658345d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1688/1688 [==============================] - 72s 42ms/step - loss: 0.1225 - accuracy: 0.9629 - val_loss: 0.0841 - val_accuracy: 0.9748\n",
      "Epoch 2/2\n",
      "1688/1688 [==============================] - 69s 41ms/step - loss: 0.0425 - accuracy: 0.9864 - val_loss: 0.0561 - val_accuracy: 0.9837\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2122e1d2a70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=xTrain,y=yTrain,batch_size=32,epochs=2,validation_data=(xDev,yDev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12d0531f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 9ms/step - loss: 0.0449 - accuracy: 0.9856\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.04494915530085564, 0.9855999946594238]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=xTest,y=yTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf14757",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "The technique (3 filters, flatten, 2 dense layers) was because I was entrigued by the talk during class. I wanted to see the filters in action. With an accuracy of 98.6, the filtering choice was 100% worth it! Also I used 2 dense layers because I imagine those dense layers as the original 2 layer NN and the 3 filters as a seperate 3 layer NN.\n",
    "\n",
    "I chose not to use regularization thanks to how clean the data is. There is no noise nor much overfitting as long as I keep the batches small. I chose to add ADAM optimization only because it worked well in my homework 1 and knew it would accelerate the speed of my backprop. \n",
    "\n",
    "## Task 4\n",
    "\n",
    "I'm going to use a linear model using scikit. This is because a linear model is a good baseline to compare most other models between. Also, its not something I've done in HW1 or HW2.\n",
    "\n",
    "It should be mentioned all that data manipulation is not needed here! We can use the 784 size array and 0-9 value yDat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "630e8a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-gather xTest & yTest\n",
    "xDat, yDat = data.load_training()\n",
    "xTest, yTest = data.load_testing()\n",
    "\n",
    "# add x_0 column\n",
    "xDat = np.append(np.ones((np.size(xDat,0),1)),xDat,axis=1)\n",
    "xTest = np.append(np.ones((np.size(xTest,0),1)),xTest,axis=1)\n",
    "\n",
    "# Normalize\n",
    "xDat = np.array(xDat) / 255.0\n",
    "xTest = np.array(xTest) / 255.0\n",
    "\n",
    "# Fit\n",
    "clf = LinearRegression().fit(xDat, yDat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06f9238d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2595\n"
     ]
    }
   ],
   "source": [
    "# Accuracy check\n",
    "pred = clf.predict(xTest)\n",
    "acc = 0\n",
    "for i in range(pred.size):\n",
    "    if(int(pred[i]) == yTest[i]):\n",
    "        acc += 1\n",
    "acc /= pred.size\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e20e1d",
   "metadata": {},
   "source": [
    "25% accuracy is actually much worse than I thought it would achieve. Of course, with more data cleaning and perhaps some regularization the accuracy would be much better. \n",
    "\n",
    "It should be mentioned instead of having 10 seperate outputs, it's just 1 output in a range from 0 to 9. That range includes values in between whole numbers meaning the output needs to be floored!\n",
    "\n",
    "Another observation is that without regularization there could be a heavy reliance on certain pixels which are not correlated to digits outside of train. DNN works around this by using filters which add context to pixels and their surroundings, improving any overfitting issues.\n",
    "\n",
    "Lastly, linear regressing on all 9 is not what linreg is best at. It would have been better to compare 2 numbers at a time, comparing all pairs of numbers. In this sense, perhaps a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc77c498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
